# Multiple regression

## Multiple regression

<small>

$$\LARGE{\text{outcome} = \text{model} + \text{error}}$$

In statistics, linear regression is a linear approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables denoted X.

$$\LARGE{Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dotso + \beta_n X_{ni} + \epsilon_i}$$

In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters $\beta$'s are estimated from the data.

Source: [wikipedia](https://en.wikipedia.org/wiki/Linear_regression)

</small>

## Outcome vs Model

```{r, fig.align='center', fig.asp=.5, echo=TRUE}
#| output-location: slide

error = c(2, 1, .5, .1)
n = 100

layout(matrix(1:4,1,4))
for(e in error) {
  
  x = rnorm(n)
  y = x + rnorm(n, 0 , e)
  
  r   = round(cor(x,y), 2)
  r.2 = round(r^2, 2)
  
  plot(x,y, las = 1, ylab = "outcome", xlab = "model", main = paste("r =", r," r2 = ", r.2), ylim=c(-2,2), xlim=c(-2,2))
  fit <- lm(y ~ x)
  abline(fit, col = "red")
  
}

```

## Assumptions

A selection from Field (8.3.2.1. Assumptions of the linear model):

For simple regression

-   Sensitivity
-   Homoscedasticity

Plus multiple regressin

-   Multicollinearity
-   Linearity

## Sensitivity

Outliers

-   Extreme residuals
    -   Cook's distance (\< 1)
    -   Mahalonobis (\< 11 at N = 30)
    -   Laverage (The average leverage value is defined as (k + 1)/n)

## 

```{r, fig.align='center', fig.width=5, fig.height=5, echo=FALSE}
x[1] =  2
y[1] = -2

plot(x,y, 
     las  = 1, 
     ylab = "outcome", 
     xlab = "model", 
     main = paste("r =", r," r2 = ", r.2), 
     ylim = c(-2,2), 
     xlim = c(-2,2))
  
abline(fit, col = "red")

fit2 <- lm(y ~ x)
abline(fit2, col = "blue")
  
r   = round(cor(x,y), 2)
r.2 = round(r^2, 2)

text(x[1], y[1], paste("r =", r," r2 = ", r.2), pos = 2, col = "blue")
```

## Homoscedasticity

::: {.columns}
::: {.column}
-   Variance of residual should be equal across all expected values
    -   Look at scatterplot of standardized: expected values $\times$ residuals. Roughly round shape is needed.
:::

::: {.column}
```{r, fig.align='center', fig.width=5, fig.height=5, echo=TRUE}
#| code-fold: true
#| code-line-numbers: "5-6"
set.seed(27364)

fit <- lm(x[2:n] ~ y[2:n])

ZPRED  = scale(fit$fitted.values)
ZREDID = scale(fit$residuals)

plot(ZPRED, ZREDID)
abline(h = 0, v = 0, lwd=2)

#install.packages("plotrix")
if(!"plotrix" %in% installed.packages()) { install.packages("plotrix") };
library("plotrix")
draw.circle(0,0,1.7,col=rgb(1,0,0,.5))
```
:::
:::

## Multicollinearity

::: {.columns}
::: {.column width="60%"}

To adhere to the multicollinearity assumptien, there must not be a too high linear relation between the predictor variables.

This can be assessed through:

-   Correlations
-   Matrix scatterplot <!-- * VIF: max < 10, mean < 1 -->
-   Tolerance \> 0.2
:::

::: {.column  width="40%"}
![](../../../../topics/regression_multiple_predictors/pinguins.jpg){width="350px"}
:::
:::

## Linearity

For the linearity assumption to hold, the predictors must have a linear relation to the outcome variable.

This can be checked through:

-   Correlations
-   Matrix scatterplot with predictors and outcome variable

## Example {.center}

> Perdict study outcome based on IQ and motivation.

## Read data {.smaller}

```{r, echo=FALSE, eval=FALSE, warning=FALSE, message=FALSE}
set.seed(273645)
n = 89

latent.variable = rnorm(n, 0, 1) # Brain speed?

IQ        = 120 + ( latent.variable * 15 ) + rnorm(n)
motivation = 15 + ( latent.variable * 2  ) + rnorm(n)

b_0 = -100
b_1 = 1 # Beta for IQ̧
b_2 = 1 # Beta for Motivation

error = rnorm(n, 0, 5)

study.outcome = b_0 + b_1 * IQ + b_2 * motivation + error

data = data.frame(study.outcome, motivation, IQ)

fit = lm(study.outcome ~ motivation + IQ, data)
summary(fit)

head(data, 10)
```

```{r, warning=FALSE, message=FALSE, echo=TRUE, eval=TRUE}

# data <- read.csv('IQ.csv', header=T)
data <- read.csv('IQ.csv', header=TRUE)

head(data)

IQ            = data$IQ
study.outcome = data$Studieprestatie
motivation    = data$Motivatie
```

## Regression model in R

Perdict study outcome based on IQ and motivation.

```{r, echo=TRUE}
fit <- lm(study.outcome ~ IQ + motivation)
```

## What is the model {.smaller}

```{r, echo=TRUE}
fit$coefficients

b.0 <- round(fit$coefficients[1], 2) ## Intercept
b.1 <- round(fit$coefficients[2], 2) ## Beta coefficient for IQ
b.2 <- round(fit$coefficients[3], 2) ## Beta coefficient for motivation
```

De beta coëfficients are:

-   $b_0$ (intercept) = `r b.0`
-   $b_1$ = `r b.1`
-   $b_2$ = `r b.2`.

## Model summaries

```{r, echo=TRUE}
summary(fit)
```

## Visual

```{r, echo=FALSE, warning=FALSE}
## 3d plot package rgl
## install.packages('rgl')
# if(!"rgl" %in% installed.packages()) { install.packages("rgl") };
# library("rgl")
# 
# library(knitr)
# knit_hooks$set(webgl = hook_webgl)



#1 #D scatter 

# plot3d(IQ, motivation, study.outcome,
#        col  = "red",
#        size = 8)

#2 Add IQ value planes 

quantiles <- as.vector(quantile(IQ,seq(.1,.9,.1)))
sds       <- c(mean(motivation)+(sd(motivation)*c(-1,0,1)))  
sds       <- 1:4

# planes3d(a = 0,
#          b = 1,
#          c = 0,
#          #d = c(1,2,3,4),
#          d = -sds,
#          alpha=0.1,
#          color = c("blue"))

#3 Add regression model surface

## Fit model
fit <- lm(study.outcome ~ IQ + motivation)

## Create xyz coordinates
regeq <- function(IQ, motivation) { 
    fit$coefficients[1] + 
    fit$coefficients[2]*IQ + 
    fit$coefficients[3]*motivation
}


x.pre <- seq(120, 135,length.out=30)
y.mod <- seq( 0,   5 ,length.out=30)

z.pre <- outer(x.pre, y.mod, FUN='regeq')

# z.pre[z.pre > 4] = 4
# z.pre[z.pre < 1] = 1


## Add 3D regression plane to scatter plot 
# surface3d(x.pre, y.mod, z.pre, color = c("green"))
# 
# aspect3d(1,1,1)
# # view3d(theta = 5)
# # view3d(theta = -10, phi = -90)
# # play3d(spin3d(axis = c(0, 0, 1), rpm = 30), duration = 5)
# 
# subid <- currentSubscene3d()
# rglwidget(elementId="plot3drgl2")

```



<!-- Lef -->

<!-- </button> -->

<!-- <button type="button" onclick="rotate(10)"> -->

<!-- Right -->

<!-- </button> -->

## What are the expected values based on this model

$$\widehat{\text{studie prestatie}} = b_0 + b_1 \text{IQ} + b_2 \text{motivation}$$

```{r, echo=TRUE}
exp.stu.prest <- b.0 + b.1 * IQ + b.2 * motivation

model <- exp.stu.prest
```

$$\text{model} = \widehat{\text{studie prestatie}}$$

## Apply regression model

$$\widehat{\text{studie prestatie}} = b_0 + b_1 \text{IQ} + b_2 \text{motivation}$$ $$\widehat{\text{model}} = b_0 + b_1 \text{IQ} + b_2 \text{motivation}$$

```{r, echo=TRUE}
cbind(model, b.0, b.1, IQ, b.2, motivation)[1:5,]
```

$$\widehat{\text{model}} = `r b.0` + `r b.1` \times \text{IQ} + `r b.2` \times \text{motivation}$$

## How far are we off?

```{r, echo=TRUE}
error <- study.outcome - model

cbind(model, study.outcome, error)[1:5,]
```

## Outcome = Model + Error

Is that true?

```{r, echo=TRUE}
study.outcome == model + error
```

> -   Yes!

## Visual {.smaller}

```{r, echo=TRUE}
#| output-location: slide

plot(study.outcome, xlab='personen', ylab='study.outcome')

n <- length(study.outcome)
gemiddelde.study.outcome <- mean(study.outcome)

## Voeg het gemiddelde toe
lines(c(1, n), rep(gemiddelde.study.outcome, 2))

## Wat is de totale variantie?
segments(1:n, study.outcome, 1:n, gemiddelde.study.outcome, col='blue')

## Wat zijn onze verwachte scores op basis van dit regressie model?
points(model, col='orange')

## Hoever zitten we ernaast, wat is de error?
segments(1:n, study.outcome, 1:n, model, col='purple', lwd=3)
```

## Explained variance {.smaller}

The explained variance is the deviation of the estimated model outcome compared to the total mean.

To get a percentage of explained variance, it must be compared to the total variance. In terms of squares:

$$\frac{{SS}_{model}}{{SS}_{total}}$$

We also call this: $r^2$ of $R^2$.

```{r, echo=TRUE}
r <- cor(study.outcome, model)
r^2
```

## Compare models

```{r, echo=TRUE}
fit1 <- lm(study.outcome ~ motivation, data)
fit2 <- lm(study.outcome ~ motivation + IQ, data)

# summary(fit1)
# summary(fit2)

anova(fit1, fit2)
```
