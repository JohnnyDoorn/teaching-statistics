# ANCOVA

```{r, echo=FALSE}
# Clear memory
rm(list=ls())

if(!"ggplot2" %in% installed.packages()) { install.packages("ggplot2") }
library("ggplot2")

if(!"DT" %in% installed.packages()) { install.packages("DT") }
library("DT")

set.seed(1976)
# Simulate data
n <- 21
k <- 3
nationality <- factor(rep(1:3, each = n/k))
# nationality <- factor(nationality)
levels(nationality) <- c("Dutch", "German", "Belgian")
# nationality <- as.factor(rep(c("Dutch", "German", "Belgian"), each = n/k))
mu.covar    <- 8
sigma.covar <- 1
openness       <- round(rnorm(n,mu.covar,sigma.covar),2)

# Create dummy variables
dummy.1 <- ifelse(nationality == "German", 1, 0)
dummy.2 <- ifelse(nationality == "Belgian", 1, 0)

# Set parameters
b.0 <- 25 # initial value for group 1
b.1 <- -1.5  # difference between group 1 and 2
b.2 <- -4  # difference between group 1 and 3
b.3 <- 2  # Weight for covariate

# Create error
error <- rnorm(n,0,2)
```

## ANCOVA {.section}

Determine main effect while accounting for covariate

* 1 dependent variable
* 1 or more independent variables
* 1 or more covariates

A covariate is a variable that can influence the DV. By adding a covariate, we reduce error/residual in the model.

## Assumptions

* Same as ANOVA
* Independence of the covariate and treatment effect ยง12.5.1.
    * No difference on ANOVA with covar and independent variable
    * Matching experimental groups on the covariate
* Homogeneity of regression slopes ยง12.5.2.
    * Visual: scatterplot dep var * covar per condition
    * Testing: interaction indep. var * covar

## Independence of the covariate and treatment effect (Fig 12.2)

<img src="../../images/FieldCovWarning.png" style="width:40%;"/>

## Data example

We want to test the difference in extraversion but want to also account for openness to experience.

* Dependent variable: Extraversion
* Independent variabele: Nationality (#groups $= k = 3$)
    + Dutch
    + German
    + Belgian
* Covariate: Openness to experience


## Define the model {.subsection}

$${extraversion} = {model} + {error}$$

${model} = {independent} + {covariate}$
$\color{white}{model} = {nationality} + {openness}$

$\color{white}{model}$


Linear model with $k-1$ dummy variables:

$$\hat{y} = b_0 + b_1 {dummy}_1 + b_2 {dummy}_2 + b_3 covar$$


```{r, echo=FALSE}
# Define model
extraversion <- b.0 + b.1 * dummy.1 + b.2 * dummy.2 + b.3 * openness + error
```

## The data {.smaller}

```{r, echo=FALSE}
extraversion <- round(extraversion,2)
error <- round(error,2)

n <- 1:length(extraversion)

dummies <- data.frame(nationality, dummy.1, dummy.2, openness, extraversion)
# put data in data frame
data <- data.frame(nationality, openness, extraversion)

# Order by group
data <- data[order(as.numeric(data$nationality)),]
data <- cbind(n, data)
# Write data for use in SPSS
write.csv(data, "ANCOVA_OpenNationality.csv", row.names=FALSE)
plotData <- data
datatable(data, 
          extensions = 'Buttons',
          options    = list(searching = FALSE, 
                            scrollY   = 300, 
                            paging    = F, 
                            info      = F,
                            dom       = 'Bfrtip',
                            buttons   = c('csv')),
)
```



## Dummies {.smaller}

```{r, echo=FALSE}
datatable(dummies, options = list(searching = FALSE, scrollY = 435, paging = FALSE, info = FALSE))
```

## Observed group means {.subsection}

```{r, echo=TRUE}
aggregate(extraversion ~ nationality, data, mean)
```

## Model fit without covariate {.smaller}

What are the beta coefficients when we fit a model that only has `nationality` as a predictor variable?

```{r, echo=TRUE, }
fit.group <- lm(extraversion ~ nationality, data); fit.group$coefficients
```

- $\beta_{0} = `r  round(fit.group$coefficients[1], 2)`$ 

- $\beta_{German} = `r round(fit.group$coefficients[2], 2)`$

    - Prediction for German: `r  round(fit.group$coefficients[1], 2)` + `r round(fit.group$coefficients[2], 2)` = `r round(round(fit.group$coefficients[1], 2) + round(fit.group$coefficients[2], 2), 2)`

- $\beta_{Belgian} = `r round(fit.group$coefficients[3], 2)`$

    - Prediction for Belgian: `r round(fit.group$coefficients[1], 2)` + `r round(fit.group$coefficients[3], 2)` = `r round(round(fit.group$coefficients[1], 2) + round(fit.group$coefficients[3], 2), 2)`

## Model fit with only covariate {.smaller}

What are the beta coefficients when we fit a model that only has `openness` as predictor variable?

```{r, echo=TRUE}
fit.covar <- lm(extraversion ~ openness, data); fit.covar$coefficients
```

$\beta_{0} = `r  round(fit.covar$coefficients[1],2)`$ 

$\beta_{Open} = `r  round(fit.covar$coefficients[2],2)`$ 


## Model fit with all predictor variables (factor + covariate) {.smaller}

What are the beta coefficients when we fit the full model (i.e., with both predictor variables)?

```{r, echo=TRUE}
fit <- lm(extraversion ~  nationality + openness, data); fit$coefficients
```

```{r, echo=FALSE}
fit$coefficients <- round(fit$coefficients, 2)
```

$\beta_{Dutch} = `r  fit$coefficients[1]`$

$\beta_{German} = `r fit$coefficients[2]`$

$\beta_{Belgian} = `r fit$coefficients[3]`$

$\beta_{Open} = `r fit$coefficients[4]`$

## Predictions of the full model {.smaller}

For a German with a score of 8 on Openness:

```{r, echo=TRUE}
fit <- lm(extraversion ~ nationality + openness, data); fit$coefficients
```
- $\beta_{0} = `r round(fit$coefficients[1], 2)`$
- $\beta_{German} = `r round(fit$coefficients[2], 2)`$
- $\beta_{Open} = `r round(fit$coefficients[4], 2)`$

    - Prediction for German: `r round(fit$coefficients[1],2)` + `r round(fit$coefficients[2], 2)` + 8 * `r round(fit$coefficients[4],2)` = `r round(fit$coefficients[1] + 1 * fit$coefficients[2] + 0 * fit$coefficients[3] + 
  8 * fit$coefficients[4],2)`
  
How about a Belgian with 6 Openness?

<!-- ## Total variance {.subsection} -->

<!-- What is the total variance? -->

<!-- ${MS}_{total} = s^2_{extraversion} = \frac{{SS}_{extraversion}}{{df}_{extraversion}}$ -->

<!-- ```{r, echo=TRUE} -->
<!-- ms.t <- var(data$extraversion); ms.t -->
<!-- ss.t <- var(data$extraversion) * (length(data$extraversion) - 1); ss.t -->
<!-- ``` -->

<!-- ## The data -->

```{r, echo=FALSE}
# Add grand mean to data frame
data$grand.mean <- mean(data$extraversion)

# datatable(data, options = list(searching = FALSE, scrollY = 415, paging = F, info = F))
```

## Total variance visual {.subsection}

```{r, echo=FALSE}
source("../../plotFunctionsSSR.r")
colnames(plotData) <- c("pp", "group", "cov", "dv")
plotData$group <- as.factor(plotData$group)
plotSumSquaresCov(data = plotData, whatPred = "Mean", myLimY = c(30, 50), whatDisplay = c("Segments", "Sums"), myLabY = "Extraversion")

```

## Explained by group model {.smaller}

The model that predicts only using group means:

$\hat{y} = b_0 + b_1 {dummy}_1 + b_2 {dummy}_2$

$\hat{y} = `r round(fit.group$coefficients[1], 2)` + `r round(fit.group$coefficients[2], 2)` \times {dummy}_1 + `r round(fit.group$coefficients[3], 2)` \times {dummy}_2$

## Explained by group model - visual {.subsection}

```{r, echo=FALSE}
data$model.group <- (fit.group$fitted.values)
fit.covar <- lm(extraversion ~ openness, data)
data$model.covar <- (fit.covar$fitted.values)
plotSumSquaresCov(data = plotData, sumSq = "Model", whatPred = "Group means", myLimY = c(32, 48), whatDisplay = c("Segments", "Sums"), myLabY = "Extraversion")
```

## Explained by covariate model {.subsection}

The model that predicts only using openness:

$\hat{y} = b_0 + b_3 covar$

$\hat{y} = `r round(fit.covar$coefficients[1], 2)` + `r round(fit.covar$coefficients[2], 2)` \times {Openness}$


## Explained by covariate model - visual {.subsection}

```{r, echo=FALSE}

plotSumSquaresCov(data = plotData, sumSq = "Model", whatPred = "Cov", myLimY = c(32, 48), whatDisplay = c("Segments", "Sums"), myLabY = "Extraversion")

```

## Explained by full model {.smaller}

```{r, echo=FALSE}
data$model       <- (fit$fitted.values)

# datatable(data, options = list(searching = FALSE, scrollY = 415, paging = F, info = F))
```

The model that predicts with group and covariate:

$\hat{y} = b_0 + b_1 {dummy}_1 + b_2 {dummy}_2 + b_3 covar$

$\hat{y} = `r round(fit$coefficients[1], 2)` + `r round(fit$coefficients[2], 2)` \times {dummy}_1 + `r round(fit$coefficients[3], 2)` \times {dummy}_2  + `r round(fit$coefficients[4], 2)` \times {Openness}$

## Explained by full model - visual {.subsection}


```{r, echo=FALSE}
plotSumSquaresCov(data = plotData, sumSq = "Model", whatPred = "Group means + cov", myLimY = c(32, 48), whatDisplay = c("Segments", "Sums"), myLabY = "Extraversion")

```

## Unexplained variance (group and cov) {.subsection}

```{r, echo=FALSE}
plotSumSquaresCov(data = plotData, sumSq = "Error", whatPred = "Group means + cov", myLimY = c(30, 50), whatDisplay = c("Segments", "Sums"), myLabY = "Extraversion")
```

## Divide model sum of squares {.subsection}

<img src="../../images/FieldCovWarning.png" style="width:40%;"/>

---

## Divide model sum of squares {.smaller}

Model SS of full model:

```{r, echo=FALSE}
SS.model <- with(data, sum((model - grand.mean)^2))
SS.error <- with(data, sum((extraversion - model)^2))

# Sums of squares for individual effects
SS.model.group <- with(data, sum((model.group - grand.mean)^2))
SS.model.covar <- with(data, sum((model.covar - grand.mean)^2))
```

```{r, echo=TRUE}
SS.model
```

To see what is explained by group, we subtract the Model SS of the covariate model:

```{r, echo=TRUE}
SS.group <- SS.model - SS.model.covar; SS.group ## SS.group corrected for covar
```


To see what is explained by covariate, we subtract the Model SS of the group model:

```{r, echo=TRUE}
SS.covar <- SS.model - SS.model.group; SS.covar ## SS.covar corrected for group
```




## F-ratio {.subsection}


```{r, echo=FALSE}
n <- nrow(data)
k <- 3
df.model <- k - 1
df.error <- n - k - 1

MS.model <- SS.group / df.model
MS.error <- SS.error / df.error

fStat <- MS.model / MS.error
# fStat
# 
# plotSumSquaresCov(data = plotData, sumSq = "Model", whatPred = "Group means + cov", myLimY = c(32, 48), whatDisplay = c("Segments", "Sums", "F-stat"), myLabY = "Extraversion")


```

$MS_{model-group} =  \frac{`r  round(SS.group, 3)`}{`r df.model`} = `r round(round(SS.group, 3) / df.model, 3)`$ 

$F = \frac{{MS}_{model}}{{MS}_{error}} = \frac{{SIGNAL}}{{NOISE}} =  \frac{`r  round(MS.model, 3)`}{`r round( MS.error, 3)`} = `r round(fStat, 2)`$



## $P$-value
```{r, echo=FALSE}
# Set up the plot for the F-distribution
curve(df(x, df.model, df.error), from = 0, to = 25, n = 1000, bty = "n", las = 1,
      xlab = "F-value", ylab = "Density", main = "F-distribution with P-value Region")

# Shade the upper tail (p-value region) using polygon
# Define the region to fill (upper tail, greater than fStat)
x_vals <- seq(fStat, 10, length.out = 100)
y_vals <- df(x_vals, df.model, df.error)

# Draw the polygon to fill the area under the curve
polygon(c(fStat, x_vals, 10), c(0, y_vals, 0), col = "lightblue", border = NA)

# Optionally, add a vertical line at the F-statistic
arrows(x0 = fStat, x1 = fStat, y0 = 0 , y1 = 0.6, col = "darkred", lwd = 2)
text(x = fStat, y = 0.8, paste("F =", round(fStat, 2)), cex = 1.2)
text(x = fStat, y = 0.7, paste("p-val =", round(pf(fStat, df.model, df.error, lower.tail = FALSE), 4)), cex = 1.2)
```

## Alpha & Power {.smaller}
Power becomes quite abstract when we increase the complexity (i.e., number of predictors) of our models. We can make an F-distribution that symbolizes the alternative distribution by shifting the distribution more to the right (although the interpretability becomes pretty murky..)

```{r, echo=FALSE}
fValues <- seq(0, 30, .01)

curve(df(x, df.model, df.error), from = 0, to = 25, n = 1000, bty = "n", las = 1,
      xlab = "F-value", ylab = "Density", main =  "H0 and HA F-distribution")

critical.value = qf(.95, df.model, df.error)

critical.range = seq(critical.value, 30, .01)

polygon(c(critical.range,rev(critical.range)), 
        c(critical.range*0, rev(df(critical.range, df.model, df.error, ncp = 15))), col = "darkgreen")

lines(fValues, df(fValues, df.model, df.error, ncp = 15))

polygon(c(critical.range,rev(critical.range)), 
        c(critical.range*0, rev(df(critical.range, df.model, df.error))), col = "purple")
```


## Adjusted/marginal means

Marginal means are estimated group means, while keeping the covariate equal across the groups

> What are extraversion averages in each group, if they would all score the same on openness?

See also [this blogpost](https://jasp-stats.org/2020/04/14/the-wonderful-world-of-marginal-means/)


## Adjusted/marginal means

Adjusted:

```{r, echo=FALSE}
# Add dummy variables
data$dummy.1 <- ifelse(data$nationality == "German", 1, 0)
data$dummy.2 <- ifelse(data$nationality == "Belgian", 1, 0)

# b coefficients
b.cov <- fit$coefficients["openness"];          b.int = fit$coefficients["(Intercept)"]
b.2   <- fit$coefficients["nationalityGerman"]; b.3   = fit$coefficients["nationalityBelgian"]

# Adjustment factor for the means of the independent variable
data$mean.adj <- with(data, b.int + b.cov * mean(openness) + b.2 * dummy.1 + b.3 * dummy.2)

aggregate(mean.adj ~ nationality, data, mean)
```

Observed:

```{r, echo=FALSE}
aggregate(extraversion ~ nationality, data, mean)
```

## JASP

![](../../images/JASP_logo_green.svg){width="300px"}



