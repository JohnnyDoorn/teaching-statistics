# Bayesian parameter estimation {.section}

<!-- ## Updating belief -->

<!-- > Posterior $\propto$ Likelihood $\times$ Prior -->

## What is a model?

```{r fig.align='center', out.width='90%', echo = FALSE, cache=FALSE}
tBetFun <- function(x, shape1 = 1, shape2 = 1, side = "pos") {
  ds <- dbeta(x, shape1, shape2)
  if (side == "pos") {
    ds[x<0.5] <- 0
    ds <- ds / integrate(function(x) dbeta(x, shape1, shape2), lower = 0.5, upper = 1)[[1]]
  } else if (side == "neg") {
    ds[x>0.5] <- 0
    ds <- ds / integrate(function(x) dbeta(x, shape1, shape2), lower = 0, upper = 0.5)[[1]]
  }
  return(ds)
}
par(mfrow = c(1, 2))
cols <- viridis::viridis(6)
plot(1, 1, type ="n", xlim = c(0,1), ylim = c(0,4), bty = "n", main = "Sarah's Model",
     las = 1, xlab = expression(theta), ylab = "Density")
arrows(x0 = 0.5, x1 = 0.5, y0 = 0, y1 = 4.0, lwd = 4, col = cols[1])

plot(1, 1, type ="n", xlim = c(0,1), ylim = c(0,4), bty = "n", main = "Paul's Model", 
     las = 1, xlab = expression(theta), ylab = "Density")
arrows(x0 = 0.8, x1 = 0.8, y0 = 0, y1 = 4.0, lwd = 4, col = cols[3])
```


## What does a model predict?

```{r fig.align='center', out.width='90%', echo = FALSE}
par(mfrow = c(1, 2), cex.main = 0.95)
cols <- viridis::viridis(6)
barplot(dbinom(0:10, 10, 0.5), names.arg = 0:10, xlab = "Number of heads", ylab = "Likelihood",
        main = "Likely Outcomes under Sarah's Model", col = cols[1], ylim = c(0,0.32))
barplot(c(rep(0, 8), dbinom(8, 10, 0.5)), add = TRUE, col = cols[6])

barplot(dbinom(0:10, 10, 0.8), names.arg = 0:10, xlab = "Number of heads", ylab = "Likelihood",
        main = "Likely Outcomes under Paul's Model", col = cols[3], ylim = c(0,0.32))
barplot(c(rep(0, 8), dbinom(8, 10, 0.8)), add = TRUE, col = cols[6])
```

## Model with multiple values
```{r fig.align='center', out.width='90%', echo = FALSE}
par(mfrow = c(1, 2), cex.main = 0.95)
cols <- viridis::viridis(6)
plot(1, 1, type ="n", xlim = c(0,1), ylim = c(0,4), bty = "n", main = "Alex's Model",
     las = 1, xlab = expression(theta), ylab = "Density")
# curve(tBetFun(x, 1, 1), add = TRUE, col = cols[4], lwd = 4)
mySeq <- seq(0, 1, length.out = 1e3)
polygon(x =  c(mySeq, rev(mySeq)), y = c(rep(0, 1e3), tBetFun(mySeq, 1, 1, side = "neutral")), col = cols[4]) 

barplot(rep(1/11, 11), names.arg = 0:10, xlab = "Number of heads", ylab = "Likelihood",
        main = "Likely Outcomes under Alex's Model", col = cols[4], ylim = c(0,0.32))
barplot(c(rep(0, 8), 1/11), add = TRUE, col = cols[6])
```

## One-sided model

```{r fig.align='center', out.width='90%', echo = FALSE}
par(mfrow = c(1, 2), cex.main = 0.95)
cols <- viridis::viridis(6)
plot(1, 1, type ="n", xlim = c(0,1), ylim = c(0,4), bty = "n", main = "Betty's Model",
     las = 1, xlab = expression(theta), ylab = "Density")
# curve(tBetFun(x, 1, 1), add = TRUE, col = cols[4], lwd = 4)
mySeq <- seq(0.5, 1, length.out = 1e3)
polygon(x =  c(mySeq, rev(mySeq)), y = c(rep(0, 1e3), tBetFun(mySeq, 1, 1)), col = cols[2]) 
sampsU <- c(0:5, rbinom(1e5, 10, runif(1e5, 0.5, 1)))
barplot(table(sampsU)/1e5, names.arg = 0:10, xlab = "Number of heads", ylab = "Likelihood",
        main = "Likely Outcomes under Betty's Model", col = cols[2], ylim = c(0,0.32))
barplot(c(rep(0, 8), (table(sampsU)/1e5)[9]), add = TRUE, col = cols[6])


```

## Prior distribution

You have assigned a prior probability distribution to the parameter $\theta$.

> This is your prior

Now we normally do not draw our priors, but we could. 

## Priors

We can choose a flat prior, or a beta distributed prior with different parameter values $a$ and $b$.

```{r, echo=TRUE}
#| output-location: slide

theta = seq(0,1, .001)
layout(matrix(1:4,2,2))
plot(theta, dunif(theta),           type="l", ylab = "likelihood")
plot(theta, dbeta(theta, 3, 5),     type="l", ylab = "likelihood")
plot(theta, dbinom(25, 100, theta), type="l", ylab = "likelihood")
plot(theta, dbeta(theta, 2, 2),     type="l", ylab = "likelihood")
```

## Choose prior

Binomial distribution

$\theta^k (1-\theta)^{n-k} \\
\theta^{25} (1-\theta)^{100-25}$

## Now what is the data saying

### My ten tosses

$\begin{aligned}
  k &= 2 \\
  n &= 10
  \end{aligned}$

```{r, echo=TRUE}
k = 2
n = 10
```

## Likelihood

What is the most likely parameter value $\theta$ assuming the data to be true:

$\theta = \frac{2}{10} = `r k/n`$

```{r, echo=TRUE}
theta.given.data = k/n

theta.given.data
```

## Likelihood function

How likely is 2 out of 10 for all possible $\theta$ values?

$\theta^k (1-\theta)^{n-k}$

```{r, echo=TRUE}
#| output-location: slide

thetas = seq(0, 1, .01)

likelihood =  dbinom(k, n, thetas)

plot(thetas, dbinom(k, n, thetas),
     main = "Likelihood function",
     type='l', 
     ylab = "Likelihood", 
     las = 1)
abline(v=theta.given.data, lty='dashed')
```

-----

![](../../../../topics/bayes/likelihood_function.gif)

## Posterior

Now we can update our belief about the possible values of theta based on the data (the likelihood function) we found. For this we use Bayes rule.

$\begin{aligned}
  {Posterior} &\propto {Likelihood} \times {Prior} \\
  \theta^{27}(1-\theta)^{83} &= \theta^{2} (1-\theta)^{10-2} \times \theta^{25} (1-\theta)^{100-25}
  \end{aligned}$

## Visual

```{r, echo=FALSE}
layout(matrix(1:3,1,3))
plot(theta, dbinom(27, 83,  theta), type="l", ylab = "likelihood", main = "Posterior")
plot(theta, dbinom(2, 10,   theta), type="l", ylab = "likelihood", main = "Likelihood")
plot(theta, dbinom(25, 100, theta), type="l", ylab = "likelihood", main = "Prior")
```


## Theta all mighty

The true value of $theta$ for our binomial distribution.

$\Huge \theta = .68$

The data driver!

## Animation code

```{r, fig.show='animate', ffmpeg.format='gif', dev='jpeg', interval=.1, eval=FALSE, echo=TRUE}

set.seed(25)
## Run multiple samples with our real theta of .68 as our driving force.
real.theta = .68

old.k = 27
old.n = 83

for(i in 1:20) {
  
  # Choose a random sample size between 10 and 100
  sample.size.n = sample(30:100, 1)
  # Sample number of heads based on sample size and fixed real parameter value
  number.of.heads.k = rbinom(1, sample.size.n, real.theta)
  
  # sample.size.n
  # number.of.heads.k
  
  new.k = old.k + number.of.heads.k
  new.n = old.n + sample.size.n
  
  layout(matrix(1:3,1,3))
  plot(theta, dbinom(new.k, new.n,  theta), type="l", ylab = "likelihood", main = "Posterior")
  plot(theta, dbinom(number.of.heads.k, sample.size.n,   theta), type="l", ylab = "likelihood", main = "Likelihood")
  plot(theta, dbinom(old.k, old.n, theta), type="l", ylab = "likelihood", main = "Prior")
  
  old.k = new.k
  old.n = new.n
  
}
```

## Animation

![](../../../../topics/bayes/bayes_in_action.gif)

## Take home message

* Bayesians quantify uncertainty through distributions.
* The more peaked the distribution, the lower the uncertainty.
* Incoming information continually updates our knowledge; today’s posterior is tomorrow’s prior.