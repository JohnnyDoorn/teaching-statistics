# $\chi^2$ test {.section}

Relation between categorical variables

## $\chi^2$ test {.smaller}

A "chi-squared test", also written as $\chi^2$ test, is any statistical hypothesis test wherein the sampling distribution of the test statistic is a chi-squared distribution when the null hypothesis is true. Without other qualification, 'chi-squared test' often is used as short for Pearson's chi-squared test.

Chi-squared tests are often constructed from a Lack-of-fit sum of squared errors. A chi-squared test can be used to attempt rejection of the null hypothesis that the data are independent.

Source: [wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test)

## $\chi^2$ test statistic {.subsection}

$\chi^2 = \sum \frac{(\text{observed}_{ij} - \text{model}_{ij})^2}{\text{model}_{ij}}$

### Contingency table {.smalller}

::: {.columns}

::: {.column style="transform: scale(.7);"}

$\text{observed}_{ij} = 
\begin{pmatrix}
o_{11} & o_{12} & \cdots & o_{1j} \\
o_{21} & o_{22} & \cdots & o_{2j} \\
\vdots & \vdots & \ddots & \vdots \\
o_{i1} & o_{i2} & \cdots & o_{ij} 
\end{pmatrix}$

:::

::: {.column style="transform: scale(.7);"}

$\text{model}_{ij} = 
\begin{pmatrix}
m_{11}  & m_{12} & \cdots & m_{1j} \\
m_{21}  & m_{22} & \cdots & m_{2j} \\
\vdots  & \vdots & \ddots & \vdots \\
m_{i1}  & m_{i2} & \cdots & m_{ij} 
\end{pmatrix}$

:::

:::

## $\chi^2$ distribution {.smaller .subsection}

The $\chi^2$ distribution describes the test statistic under the assumption of $H_0$, given the degrees of freedom. 

$df = (r - 1) (c - 1)$ where $r$ is the number of rows and $c$ the number of columns.

```{r, echo=FALSE,render=TRUE}
#| output-location: slide

chi <- seq(0,8,.01)
df  <- c(1,2,3,6,8,10)
mycols <-  palette.colors(n = 7, palette = "Okabe-Ito")

plot( chi, dchisq(chi, df[1]), lwd = 2, col = mycols[1], type="l",
      main = "Chi squared distributions",
      ylab = "Density",
      ylim = c(0,1),
      xlab = "Chi squared")

lines(chi, dchisq(chi, df[2]), lwd = 2, col = mycols[2], type="l")
lines(chi, dchisq(chi, df[3]), lwd = 2, col = mycols[3], type="l")
lines(chi, dchisq(chi, df[4]), lwd = 2, col = mycols[4], type="l")
lines(chi, dchisq(chi, df[5]), lwd = 2, col = mycols[5], type="l")
lines(chi, dchisq(chi, df[6]), lwd = 2, col = mycols[6], type="l")

legend("topright", legend = paste("k =",df), col = mycols, lty = 1, bty = "n")
```


## Experiment {.flexbox .vcenter}

<a href=""><img src="qr_hngxh.png" alt="edu.nl/hngxh" /></a>


## Data {.smaller}

<small>

```{r, message=FALSE, eval=TRUE, echo=FALSE}
if(!'gsheet' %in% installed.packages()) { install.packages('gsheet') }
library("gsheet")
gResults <- gsheet2tbl('https://docs.google.com/spreadsheets/d/1I6p6wVmATt8VM0PvGt4lSbFCcV3WrTFs5FUQlG9Q4nQ/edit?usp=sharing')

gResults <- as.data.frame(gResults)[-(1:5), -1 ]

# results <- results[grep("2017|2022", results$Timestamp),]
for (i in 1:ncol(gResults)) {
  gResults[, i]   <- as.factor(gResults[, i])
}

# oddEvenResults <- data.frame(odd =  c(results[["3"]], results[["5"]]),
# even =  c(results[["4"]], results[["6"]]))
# for (i in 1:nrow(results)) {
#   results <- rbind(results, NA)
# }

# results <- cbind(results, oddEvenResults)
results <- data.frame(Animal = gResults$`Cats/dogs/other?`,
                      Lecture = gResults$`Digital or live lecture?`,
                      Personality= gResults$`Introvert or extrovert?`)


# install.packages("DT")
library("DT")
datatable(results)
```

</small>


```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
# Write data for use in SPSS
write.table(results, "chisqrData.csv", row.names=FALSE)
```

## Calculating $\chi^2$ {.subsection}

```{r, echo=TRUE}
observed <- table(results[, c("Lecture", "Personality")])
observed
```

$\text{observed}_{ij} = 
\begin{pmatrix}
`r observed[1,1]` & `r observed[1,2]` \\
`r observed[2,1]` & `r observed[2,2]` \\
\end{pmatrix}$

## Calculating the model {.subsection}

$\text{model}_{ij} = E_{ij} = \frac{\text{row total}_i \times \text{column total}_j}{n }$

```{r, echo=TRUE}
n   <- sum(observed)
totExt <- colSums(observed)[1]
totInt  <- colSums(observed)[2]
totDig  <- rowSums(observed)[1]
totLiv <- rowSums(observed)[2]

addmargins(observed)
```


## Calculating the model

$\text{model}_{ij} = E_{ij} = \frac{\text{row total}_i \times \text{column total}_j}{n }$

```{r, echo=TRUE}
modelPredictions <- matrix( c((totExt  * totDig)  / n,
                              (totExt  * totLiv)  / n,
                              (totInt  * totDig)  / n,
                              (totInt  * totLiv)  / n), 2, 2, 
                            byrow=FALSE, dimnames = dimnames(observed)
)

modelPredictions
```

$\text{model}_{ij} = 
\begin{pmatrix}
`r modelPredictions[1,1]` & `r modelPredictions[1,2]` \\
`r modelPredictions[2,1]` & `r modelPredictions[2,2]` \\
\end{pmatrix}$



## Error = Observed - Model {.subsection}

```{r, echo=TRUE}
observed

modelPredictions

observed - modelPredictions
```

## Calculating $\chi^2$

$\chi^2 = \sum \frac{(\text{observed}_{ij} - \text{model}_{ij})^2}{\text{model}_{ij}}$

```{r, echo=TRUE}
# Calculate chi squared
chi.squared <- sum((observed - modelPredictions)^2 / modelPredictions)
chi.squared
```

## Testing for significance {.subsection}

$df = (r - 1) (c - 1)$

## $P$-value
```{r, echo=FALSE}
# Set up the plot for the F-distribution
df <- 1
curve(dchisq(x, df), from = 0, to = 5, n = 1000, bty = "n", las = 1,
      xlab = "Chi-squared", ylab = "Density", main = "Chi-squared distribution with P-value Region")

# Shade the upper tail (p-value region) using polygon
# Define the region to fill (upper tail, greater than fStat)
x_vals <- seq(chi.squared, 10, length.out = 100)
y_vals <- dchisq(x_vals, df)

# Draw the polygon to fill the area under the curve
polygon(c(chi.squared, x_vals, 10), c(0, y_vals, 0), col = "lightblue", border = NA)

# Optionally, add a vertical line at the F-statistic
arrows(x0 = chi.squared, x1 = chi.squared, y0 = 0 , y1 = 1.6, col = "darkred", lwd = 2)
text(x = chi.squared, y = 3, bquote(Chi^2 ~ " = " ~ .(round(chi.squared, 2))), cex = 1.2)
text(x = chi.squared, y = 2, paste("p-val =", round(pchisq(chi.squared, df), 5)), cex = 1.2)
```

## Fisher's exact test {.subsection}

Calculates exact $\chi^2$ for small samples, when the $\chi^2$-distribution does not yet suffice. 

Calculate all possible permutations.

* Cell size < 5

## Yates's correction {.subsection}

For 2 x 2 contingency tables, Yates's correction is to prevent overestimation of statistical significance for small data (at least one cell of the table has an expected count smaller than 5). Unfortunately, Yates's correction may tend to overcorrect. This can result in an overly conservative result. 

$\chi^2 = \sum \frac{ ( | \text{observed}_{ij} - \text{model}_{ij} | - .5)^2}{\text{model}_{ij}}$

```{r, echo=TRUE}
#| output-location: slide

# Calculate Yates's corrected chi squared
chi.squared.yates <- sum((abs(observed - modelPredictions) - .5)^2 / modelPredictions)
chi.squared.yates
visualize::visualize.chisq(chi.squared.yates, df, section='upper')
```

<!-- ## Likelihood ratio {.subsection} -->

<!-- Alternative to Pearson's $\chi^2$.  -->

<!-- $L \chi^2 = 2 \sum \text{observed}_{ij} ln \left( \frac{\text{observed}_{ij}}{\text{model}_{ij}} \right)$ -->

<!-- ```{r, echo=TRUE} -->
<!-- #| output-location: slide -->

<!-- # ln is natural logarithm -->
<!-- ll.ratio <- 2 * sum(observed * log(observed / modelPredictions) ); ll.ratio -->

<!-- visualize.chisq(ll.ratio, df, section='upper') -->
<!-- ``` -->

## Standardized residuals {.subsection}

$\text{standardized residuals} = \frac{ \text{observed}_{ij} - \text{model}_{ij} }{ \sqrt{ \text{model}_{ij} } }$

```{r, echo=TRUE}
(observed - modelPredictions) / sqrt(modelPredictions)
```

## Effect size {.subsection}

Odds ratio based on the observed values

```{r, echo=TRUE}
odds <- round( observed, 2); odds
```

$\begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix}$

$OR = \frac{a \times d}{b \times c} = \frac{`r odds[1,1]` \times `r odds[2,2]`}{`r odds[1,2]` \times `r odds[2,1]`} = `r (odds[1,1] * odds[2,2]) / (odds[1,2] * odds[2,1])`$

## Odds

```{r, echo =FALSE}
odds
```

The extrovert/introvert ratio for digital and live audiences:

* Digital $\text{Odds}_{EI} = \frac{ `r odds[1,1]` }{ `r odds[1,2]` }$ = `r odds[1,1] / odds[1,2]`
* Live $\text{Odds}_{EI} = \frac{ `r odds[2,1]` }{ `r odds[2,2]` }$ = `r odds[2,1] / odds[2,2]`

In the digital responses, there are +- `r round(odds[1,1] / odds[1,2], 2)` times as many extroverts than introverts.
In the live responses, there  are +- `r round(odds[2,1] / odds[2,2], 2)` times as many extroverts than introverts.

## Odds

```{r, echo =FALSE}
odds
```

Alternatively, we can look at the ratio's of digital/live for extroverts and introverts:

* Extrovert $\text{Odds}_{DL} = \frac{ `r odds[1,1]` }{ `r odds[2,1]` }$ = `r odds[1,1] / odds[2,1]`
* Introvert $\text{Odds}_{DL} = \frac{ `r odds[1,2]` }{ `r odds[2,2]` }$ = `r odds[1,2] / odds[2,2]`

For the extroverts, there are +- `r round(odds[1,1] / odds[2,1], 2)` times as many digital viewers than live viewers.
For the introverts, there are +- `r round(odds[1,2] / odds[2,2], 2)` times as many digital viewers than live viewers.

## Odds ratio

Is the ratio of these odds.

$OR = \frac{\text{digital}}{\text{live}} = \frac{`r odds[1,1] / odds[1,2]`}{`r odds[2,1] / odds[2,2]`} = \frac{\text{extrovert}}{\text{introvert}}  =  \frac{`r odds[1,1] / odds[2,1]`}{`r odds[1,2] / odds[2,2]`} = `r (odds[1,1] / odds[1,2]) / (odds[2,1] / odds[2,2])`$

For this data, extroverts were approximately `r round((odds[1,1] / odds[1,2]) / (odds[2,1] / odds[2,2]), 2)` times more likely to watch digitally, compared to introverts. The odds ratio also accounts for the scores in both conditions—watching digitally and watching live—by comparing the odds of watching digitally to live viewing across both personality types.
